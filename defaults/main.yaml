---
# The Apache Kafka version to be downloaded and installed
# kafka_download_base_url should be set to https://archive.apache.org/dist/kafka/ for older versions than the current
kafka_download_base_url: http://www-eu.apache.org/dist/kafka
kafka_version: 2.7.0
kafka_scala_version: 2.13

# The kafka user and group to create files/dirs with and for running the kafka service
kafka_user: kafka
kafka_group: kafka

kafka_root_dir: /opt
kafka_dir: "{{ kafka_root_dir }}/kafka"

# Start kafka after installation
kafka_start: yes
# Restart kafka on configuration change
kafka_restart: yes

############################# Server #############################

# The id of the broker. This must be set to a unique integer for each broker.
kafka_broker_id: 0

# The address the socket server listens on. It will get the value returned from
# java.net.InetAddress.getCanonicalHostName() if not configured.
#   FORMAT:
#     listeners = security_protocol://host_name:port
#   EXAMPLE:
#     listeners = PLAINTEXT://your.host.name:9092
#listeners=PLAINTEXT://:9092
kafka_listener_protocol: PLAINTEXT
kafka_listener_hostname: localhost
kafka_listener_port: 9092

# Hostname and port the broker will advertise to producers and consumers. If not set,
# it uses the value for "listeners" if configured.  Otherwise, it will use the value
# returned from java.net.InetAddress.getCanonicalHostName().
#advertised.listeners=PLAINTEXT://your.host.name:9092
kafka_advertised_listener_protocol: "{{ kafka_listener_protocol }}"
kafka_advertised_listener_hostname: "{{ kafka_listener_hostname }}"
kafka_advertised_listener_port: "{{ kafka_listener_port }}"

# The number of threads handling network requests
kafka_num_network_threads: 3
# The number of threads that the server uses for processing requests, which may include disk I/O
kafka_num_io_threads: 8
# The send buffer (SO_SNDBUF) used by the socket server
kafka_socket_send_buffer_bytes: 102400
# The receive buffer (SO_RCVBUF) used by the socket server
kafka_socket_receive_buffer_bytes: 102400
# The maximum size of a request that the socket server will accept (protection against OOM)
kafka_socket_request_max_bytes: 104857600

# A comma seperated list of directories under which to store log files
kafka_log_dirs: /var/lib/kafka/logs

# The default number of log partitions per topic. More partitions allow greater
# parallelism for consumption, but this will also result in more files across
# the brokers.
kafka_num_partitions: 1

# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
# This value is recommended to be increased for installations with data dirs located in RAID array.
kafka_num_recovery_threads_per_data_dir: 1

# The replication factor for the group metadata internal topics "__consumer_offsets" and "__transaction_state"
# For anything other than development testing, a value greater than 1 is recommended for to ensure availability such as 3.
kafka_offsets_topic_replication_factor: 1
kafka_transaction_state_log_replication_factor: 1
kafka_transaction_state_log_min_isr: 1

# The minimum age of a log file to be eligible for deletion
kafka_log_retention_hours: 168

# The maximum size of a log segment file. When this size is reached a new log segment will be created.
kafka_log_segment_bytes: 1073741824

# The interval at which log segments are checked to see if they can be deleted according
# to the retention policies
kafka_log_retention_check_interval_ms: 300000

# Enable auto creation of topic on the server
kafka_auto_create_topics_enable: false

# Enables delete topic. Delete topic through the admin tool will have no
# effect if this config is turned off
kafka_delete_topic_enable: true

# Default replication factor for automatically created topics.
kafka_default_replication_factor: 1

kafka_group_initial_rebalance_delay_ms: 0

############################# Zookeeper #############################

# Zookeeper connection string (see zookeeper docs for details).
# This is a comma separated host:port pairs, each corresponding to a zk
# server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".
# You can also append an optional chroot string to the urls to specify the
# root directory for all kafka znodes.
kafka_zookeeper_connect: "localhost:2181"

# Timeout in ms for connecting to zookeeper
kafka_zookeeper_connection_timeout_ms: 6000

# the directory where the snapshot is stored.
kafka_zookeeper_data_dir: /tmp/zookeeper
# the port at which the clients will connect
kafka_zookeeper_client_port: 2181
# disable the per-ip limit on the number of connections since this is a non-production config
kafka_zookeeper_max_client_cnxns: 0

########################### Kafka Connect #############################

kafka_connect_bootstrap_servers: "localhost:9092"
kafka_connect_group_id: connect-cluster
kafka_connect_key_converter: org.apache.kafka.connect.json.JsonConverter
kafka_connect_value_converter: org.apache.kafka.connect.json.JsonConverter
kafka_connect_key_converter_schemas_enable: true
kafka_connect_value_converter_schemas_enable: true
kafka_connect_internal_key_converter: org.apache.kafka.connect.json.JsonConverter
kafka_connect_internal_value_converter: org.apache.kafka.connect.json.JsonConverter
kafka_connect_internal_key_converter_schemas_enable: false
kafka_connect_internal_value_converter_schemas_enable: false
kafka_connect_offset_storage_replication_factor: 1
kafka_connect_config_storage_replication_factor: 1
kafka_connect_status_storage_replication_factor: 1
kafka_connect_offset_flush_interval_ms: 10000
kafka_connect_plugin_path: /usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors

kafka_connect_offset_storage_file_filename: /tmp/connect.offsets

############################# Producer #############################

kafka_producer_bootstrap_servers: "localhost:9092"
kafka_producer_compression_type: none

############################# Consumer #############################

kafka_consumer_group_id: kafka-consumer-group
kafka_consumer_bootstrap_servers: "localhost:9092"
