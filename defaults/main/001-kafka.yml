---
# The Apache Kafka version to be downloaded and installed
# kafka_download_base_url should be set to https://archive.apache.org/dist/kafka/ for older versions than the current
kafka_download_base_url: https://downloads.apache.org/kafka
kafka_download_validate_certs: yes
kafka_version: 3.8.1
kafka_scala_version: 2.13

# The kafka user and group to create files/dirs with and for running the kafka service
kafka_create_user_group: true
kafka_user: kafka
kafka_group: kafka

kafka_root_dir: /opt
kafka_dir: "{{ kafka_root_dir }}/kafka"

# The application log folder (e.g: server.log)
kafka_log_dir: /var/log/kafka

# Start kafka after installation
kafka_start: yes
# Restart kafka on configuration change
kafka_restart: yes

############################# Server #############################

# The id of the broker. This must be set to a unique integer for each broker.
kafka_broker_id: 0

# The Java heap size (memory) allocation (xmx, xms)
kafka_java_heap: "-Xms1G -Xmx1G"

# The number of threads to use for various background processing tasks
kafka_background_threads: 10

# The address the socket server listens on. It will get the value returned from
# java.net.InetAddress.getCanonicalHostName() if not configured.
#   FORMAT:
#     listeners = security_protocol://host_name:port
#   EXAMPLE:
#     listeners = PLAINTEXT://your.host.name:9092
#listeners=PLAINTEXT://:9092
kafka_listeners:
  - "PLAINTEXT://:9092"

# Hostname and port the broker will advertise to producers and consumers. If not set,
# it uses the value for "listeners" if configured.  Otherwise, it will use the value
# returned from java.net.InetAddress.getCanonicalHostName().
#advertised.listeners=PLAINTEXT://your.host.name:9092
# kafka_advertised_listeners:
#   - "SASL_SSL://:9094"
#   - "PLAINTEXT://:9092"

# The number of threads handling network requests
kafka_num_network_threads: 3
# The number of threads that the server uses for processing requests, which may include disk I/O
kafka_num_io_threads: 8
# Specify the number of threads that are used to replicate messages from a source broker. Increasing this value can lead to increased parallelism in I/O operations in the broker.
kafka_num_replica_fetchers: 1

# The send buffer (SO_SNDBUF) used by the socket server
kafka_socket_send_buffer_bytes: 102400
# The receive buffer (SO_RCVBUF) used by the socket server
kafka_socket_receive_buffer_bytes: 102400
# The maximum size of a request that the socket server will accept (protection against OOM)
kafka_socket_request_max_bytes: 104857600
# The socket receive buffer for network requests
kafka_replica_socket_receive_buffer_bytes: 65536

# A comma separated list of directories under which to store data log files
kafka_data_log_dirs: /var/lib/kafka/logs

# The default number of log partitions per topic. More partitions allow greater
# parallelism for consumption, but this will also result in more files across
# the brokers.
kafka_num_partitions: 1

# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
# This value is recommended to be increased for installations with data dirs located in RAID array.
kafka_num_recovery_threads_per_data_dir: 1

# The number of background threads to use for log cleaning
kafka_log_cleaner_threads: 1

# The replication factor for the group metadata internal topics "__consumer_offsets" and "__transaction_state"
# For anything other than development testing, a value greater than 1 is recommended for to ensure availability such as 3.
kafka_offsets_topic_replication_factor: 1
kafka_transaction_state_log_replication_factor: 1
kafka_transaction_state_log_min_isr: 1

# The minimum age of a log file to be eligible for deletion
kafka_log_retention_hours: 168

# The maximum size of a log segment file. When this size is reached a new log segment will be created.
kafka_log_segment_bytes: 1073741824

# The interval at which log segments are checked to see if they can be deleted according
# to the retention policies
kafka_log_retention_check_interval_ms: 300000

# Enable auto creation of topic on the server
kafka_auto_create_topics_enable: false

# Enables delete topic. Delete topic through the admin tool will have no
# effect if this config is turned off
kafka_delete_topic_enable: true

# Default replication factor for automatically created topics.
kafka_default_replication_factor: 1

kafka_group_initial_rebalance_delay_ms: 0

############################# Zookeeper #############################

# Zookeeper connection string (see zookeeper docs for details).
# This is a comma separated host:port pairs, each corresponding to a zk
# server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".
# You can also append an optional chroot string to the urls to specify the
# root directory for all kafka znodes.
kafka_zookeeper_connect: "localhost:2181"

# the directory where the snapshot is stored.
kafka_zookeeper_data_dir: /tmp/zookeeper
# the port at which the clients will connect
kafka_zookeeper_client_port: 2181
# disable the per-ip limit on the number of connections since this is a non-production config
kafka_zookeeper_max_client_cnxns: 0

############################# Timeout #############################

# Timeout in ms for connecting to zookeeper
kafka_zookeeper_connection_timeout_ms: 6000

# Offset commit will be delayed until all replicas for the offsets topic receive the commit or this timeout is reached. This is similar to the producer request timeout.
kafka_offsets_commit_timeout_ms: 5000

# Max wait time for each fetcher request issued by follower replicas. This value should always be less than the replica.lag.time.max.ms at all times to prevent frequent shrinking of ISR for low throughput topics
kafka_replica_fetch_wait_max_ms: 500

# The amount of time to sleep when there are no logs to clean
kafka_log_cleaner_backoff_ms: 15000

########################### Kafka Connect #############################

kafka_connect_bootstrap_servers: "localhost:9092"
kafka_connect_group_id: connect-cluster
kafka_connect_key_converter: org.apache.kafka.connect.json.JsonConverter
kafka_connect_value_converter: org.apache.kafka.connect.json.JsonConverter
kafka_connect_key_converter_schemas_enable: true
kafka_connect_value_converter_schemas_enable: true
kafka_connect_internal_key_converter: org.apache.kafka.connect.json.JsonConverter
kafka_connect_internal_value_converter: org.apache.kafka.connect.json.JsonConverter
kafka_connect_internal_key_converter_schemas_enable: false
kafka_connect_internal_value_converter_schemas_enable: false
kafka_connect_offset_storage_replication_factor: 1
kafka_connect_config_storage_replication_factor: 1
kafka_connect_status_storage_replication_factor: 1
kafka_connect_offset_flush_interval_ms: 10000
kafka_connect_plugin_path: /usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors

kafka_connect_offset_storage_file_filename: /tmp/connect.offsets

############################# Producer #############################

kafka_producer_bootstrap_servers: "localhost:9092"
kafka_producer_compression_type: none

############################# Consumer #############################

kafka_consumer_group_id: kafka-consumer-group
kafka_consumer_bootstrap_servers: "localhost:9092"

# Example authentication
############################# AUTHENTICATION #############################
# kafka_listener_auth:
#   - name: "listener.name.sasl_ssl.plain.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule"
#     username: "admin"
#     password: "admin-secret"
#     users:
#       - user: "test1"
#         pass: "test1-secret"
#       - user: "test2"
#         pass: "test2-secret"
#
# kafka_sasl_enabled_mechanisms: "PLAIN"
# kafka_ssl:
#   cipher_suites: "TLS_DHE_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384"
#   truststore_location: "/etc/security/broker.keystore.jks"
#   truststore_password: "secret"
#   keystore_location: "/etc/security/broker.keystore.jks"
#   keystore_password: "secret"
#   key_password: "secret"
#   client_auth: "required"

# Here you can define custom opts e.g. settings for JMX Exporter
# kafka_opts: ""